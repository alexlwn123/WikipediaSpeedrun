{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Info Retrieval Project",
      "provenance": [],
      "authorship_tag": "ABX9TyPETRjfQA4Fe2p/qRGB46pY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlwn123/WikipediaSpeedrun/blob/master/Info_Retrieval_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odw5Du30yBJA",
        "outputId": "0d0abb0c-b574-4e6f-8fc3-c53e64fe8cf2"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "!pip install wikipedia-api\n",
        "import wikipediaapi as wikipedia\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import concurrent.futures\n",
        "from hashlib import md5\n",
        "import sys\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/3d/289963bbf51f8d00cdf7483cdc2baee25ba877e8b4eb72157c47211e3b57/Wikipedia-API-0.5.4.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from wikipedia-api) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->wikipedia-api) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia-api\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-cp37-none-any.whl size=13462 sha256=df5c51173dadc269d9baa217adf5dce33d4211740fa3bd0765e5e5e90d3da836\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/40/42/ba1d497f3712281b659dd65b566fc868035c859239571a725a\n",
            "Successfully built wikipedia-api\n",
            "Installing collected packages: wikipedia-api\n",
            "Successfully installed wikipedia-api-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWBVJ0NByQgk",
        "outputId": "ed4e556b-145e-495f-ecf2-a502a8fc9604"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "twenty = fetch_20newsgroups()\n",
        "wiki = wikipedia.Wikipedia('en')\n",
        "stopWords = stopwords.words('english') \n",
        "\n",
        "documents = []\n",
        "titles = {}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYlSvvQFyUw6"
      },
      "source": [
        "def get_page(title):\n",
        "    if ':' in link:\n",
        "        return\n",
        "    new_page = wiki.page(link)\n",
        "    if new_page.exists():\n",
        "        new_doc = clean_doc(new_page.summary)\n",
        "        return (link, new_doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__d1ZEir_to5"
      },
      "source": [
        "def clean_doc(document):\n",
        "    return (document, [word for line in document.split('\\n') for word in line.translate(str.maketrans('','', string.punctuation)).lower().split()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9N1HOayFBxG"
      },
      "source": [
        "def add_link(link):\n",
        "    if ':' in link:\n",
        "        return\n",
        "    new_page = wiki.page(link)\n",
        "    try:\n",
        "        # if new_page.exists():\n",
        "        #     # summ = new_page.summary\n",
        "        #     #titles[md5(summ.encode())] = link\n",
        "        #     # return (link, summ)\n",
        "        return (link, new_page.summary)\n",
        "    except:\n",
        "         return\n",
        "        #clean_doc(new_page.summary)\n",
        "       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72O2GNeEB0BU"
      },
      "source": [
        "def donzeed(checked, link):\n",
        "    for x in checked:\n",
        "        if link.startswith(x):\n",
        "            return True\n",
        "    return False\n",
        "def add_links(curr_page):\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:            \n",
        "        futures = []\n",
        "        results = {}\n",
        "        checked = []\n",
        "        links = [x for x in list(curr_page.links.keys()) if x not in titles.values()]\n",
        "\n",
        "        for link in links[:min(90, len(links))]:\n",
        "            if donzeed(checked, link):\n",
        "                 continue \n",
        "            #print(link, end =' ')\n",
        "            #add_link(link)\n",
        "            checked.append(link)\n",
        "            futures.append(executor.submit(add_link, link))\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            res = future.result()\n",
        "            if not res:\n",
        "                continue\n",
        "            title, summary = res[0], res[1]\n",
        "            documents.append(summary)\n",
        "            titles[md5(summary.encode()).digest()] = title\n",
        "        \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3BCUaShFZKn"
      },
      "source": [
        "def update_tfidfs(target):\n",
        "\n",
        "    tfidf = TfidfVectorizer().fit_transform(documents)\n",
        "    \n",
        "    cosine_similarities = linear_kernel(tfidf[0:1], tfidf).flatten()   \n",
        "    \n",
        "    return cosine_similarities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BRxlNB4BQFj",
        "outputId": "0689df89-55a0-4470-c909-539de4e59648"
      },
      "source": [
        "start = \"mario\"\n",
        "target = \"WWII\"\n",
        "\n",
        "import cProfile\n",
        "import pstats\n",
        "import timeit\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "start_page, target_page = wiki.page(start), wiki.page(target)\n",
        "target = target_page.title\n",
        "# targets = [target]\n",
        "targets = ['WWII', 'World War II', 'The Second World War', 'WW2']\n",
        "#targets = [\"SCP Foundation\"]\n",
        "#targets = [\"BeyoncÃ©\"]\n",
        "\n",
        "def contains_any(targets, links):\n",
        "    for target in targets:\n",
        "        if target in links:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "print(start, '-->', target)\n",
        "\n",
        "if not start_page.exists() or not target_page.exists():\n",
        "        print('Start and/or Target page does not exist.')\n",
        "        print('Exiting...')\n",
        "        sys.exit(1)\n",
        "\n",
        "titles = {}\n",
        "documents = []\n",
        "\n",
        "titles[md5(target_page.summary.encode()).digest()] = target\n",
        "titles[md5(start_page.summary.encode()).digest()] = start\n",
        "documents.append(target_page.summary)\n",
        "documents.append(start_page.summary)\n",
        " \n",
        "\n",
        "\n",
        "curr_page = start_page\n",
        "\n",
        "twenty = fetch_20newsgroups()\n",
        "#print('twenty', type(twenty.data[0]))\n",
        "\n",
        "path = [start]\n",
        "print('Before', timeit.default_timer() - start_time)\n",
        "\n",
        "last = start\n",
        "#while target not in curr_page.links:\n",
        "while not contains_any(targets, curr_page.links):\n",
        "    if timeit.default_timer() - start_time > 100:\n",
        "        print('Timeout: > 100s')\n",
        "        print('Exiting...')\n",
        "        sys.exit(1)\n",
        "    if not curr_page.exists():\n",
        "        print(curr_page, \"does not exist\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(curr_page.title,  timeit.default_timer() - start_time)\n",
        "    add_links(curr_page)\n",
        "    print('Added Links', timeit.default_timer() - start_time)\n",
        "    cosines = update_tfidfs(target)\n",
        "    print('Updated tfidfs',  timeit.default_timer() - start_time)\n",
        "    related_docs_indices = cosines.argsort()[:-5:-1]\n",
        "    print()\n",
        "# print('cosines', cosines)\n",
        "    print('indx', related_docs_indices)\n",
        "    print('top', cosines[related_docs_indices])\n",
        "    best = titles[md5(documents[related_docs_indices[1]].encode()).digest()]\n",
        "    print('Title:', best)\n",
        "    if last != best:\n",
        "        path.append(best)\n",
        "    else:\n",
        "        best = titles[md5(documents[related_docs_indices[2]].encode()).digest()]\n",
        "        path.append(best)\n",
        "    last = best\n",
        "    curr_page = wiki.page(best)\n",
        "    print('Done',  timeit.default_timer() - start_time)\n",
        "\n",
        "path.append(target)\n",
        "print('\\nPath...')\n",
        "print(' --> '.join(path))\n",
        "print('Total',  timeit.default_timer() - start_time)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mario --> WWII\n",
            "Before 6.067649393000011\n",
            "Mario 7.044223074000001\n",
            "Added Links 17.15273493899997\n",
            "Updated tfidfs 17.17381834699995\n",
            "\n",
            "indx [ 0 13 49 46]\n",
            "top [1.         0.36175756 0.31402152 0.30665044]\n",
            "Title: Association football\n",
            "Done 17.17756128999997\n",
            "\n",
            "Path...\n",
            "mario --> Association football --> WWII\n",
            "Total 18.780584567999995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay-DvaylkOKd"
      },
      "source": [
        ""
      ]
    }
  ]
}